{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65704539-af33-4714-8410-08e0f581e377",
   "metadata": {},
   "source": [
    "Q1. Explain the assumptions required to use ANOVA and provide examples of violations that could impact\n",
    "the validity of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420c668f-0029-46e6-8cea-89f6fec7bf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANOVA (Analysis of Variance) relies on several assumptions for its validity:\n",
    "\n",
    "1. Normality: The data within each group should follow a normal distribution. Violation: If the data in one or more groups significantly deviates from normality, it can affect the ANOVA results. For instance, skewed or heavily tailed distributions might impact the test.\n",
    "\n",
    "2. Homogeneity of Variance: The variance (spread) of scores in different groups should be roughly equal. Violation: Unequal variances across groups can distort the outcomes. For example, one group having significantly larger variation than others might impact the ANOVA.\n",
    "\n",
    "3. Independence: Observations within each group must be independent of each other. Violation: If observations are not independent (e.g., repeated measures or nested designs), it can affect the ANOVA results, leading to inflated or deflated significance.\n",
    "\n",
    "Examples of violations impacting validity:\n",
    "- Normality Violation: In a medical study comparing drug efficacy across groups, if the drug's side effects in one group result in highly skewed data, violating the normality assumption, it could distort the ANOVA conclusions.\n",
    "  \n",
    "- Homogeneity of Variance Violation: In an educational study comparing teaching methods across schools, if the achievement scores' variability significantly differs between schools, violating the homogeneity of variance, it could undermine the ANOVA outcomes.\n",
    "  \n",
    "- Independence Violation: In a survey where respondents within the same family provide responses, violating the independence assumption, it could introduce correlations among responses, impacting ANOVA results.\n",
    "\n",
    "Understanding these assumptions and potential violations helps ensure the reliability and interpretability of ANOVA outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c42efef-c592-4441-8680-08d91822d9b8",
   "metadata": {},
   "source": [
    "Q2. What are the three types of ANOVA, and in what situations would each be used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabd3a08-28f8-45a2-83e4-ff3539d78d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "The three types of ANOVA are:\n",
    "\n",
    "1. One-Way ANOVA: \n",
    "   - Usage: Used when comparing the means of three or more independent groups or levels within a single factor.\n",
    "   - Situation: For example, testing the impact of different teaching methods (factor: teaching method) on student performance by comparing the mean scores of students taught using various methods (levels: method A, method B, method C).\n",
    "\n",
    "2. Two-Way ANOVA:\n",
    "   - Usage: Examines the influence of two categorical independent variables (factors) on a continuous dependent variable.\n",
    "   - Situation: For instance, investigating the effect of both gender (factor 1: male/female) and diet (factor 2: diet type) on weight loss (dependent variable: weight change).\n",
    "\n",
    "3. Repeated Measures ANOVA:\n",
    "   - Usage: Analyzes the effects of a treatment or intervention over time within the same subjects/participants.\n",
    "   - Situation: Used in longitudinal studies where measurements are taken at different time points on the same individuals, like assessing the impact of a drug over various weeks on patients' blood pressure.\n",
    "\n",
    "Each type of ANOVA suits different experimental designs and research questions. One-Way ANOVA compares means across different groups on a single factor. Two-Way ANOVA extends this to two factors, examining their individual and combined effects. Repeated Measures ANOVA accounts for within-subject variations across multiple time points or conditions. Choosing the right ANOVA type hinges on the experimental setup and the variables under investigation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8ced93-7d68-402e-8bcb-61addedf2b8a",
   "metadata": {},
   "source": [
    "Q3. What is the partitioning of variance in ANOVA, and why is it important to understand this concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccb5cc8-3565-4cb1-bd14-43efff5d36e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "The partitioning of variance in ANOVA refers to the division of the total variance in the data into distinct components that correspond to different sources of variation. These components include:\n",
    "\n",
    "1. Between-Group Variance: This variance represents differences among group means. It measures the variability between the means of the groups being compared. A large between-group variance suggests that the groups differ significantly from each other.\n",
    "\n",
    "2. Within-Group Variance (or Error Variance): This variance accounts for differences within each group. It assesses the variability of individual scores around their respective group means. A smaller within-group variance indicates that observations within each group are more similar.\n",
    "\n",
    "Understanding the partitioning of variance is crucial for several reasons:\n",
    "\n",
    "1. nterpretation of Results: It helps in interpreting ANOVA results by clarifying the sources of variability contributing to differences among means. This understanding guides researchers in drawing appropriate conclusions about the significance of group differences.\n",
    "\n",
    "2. Effect Size Estimation: It aids in estimating effect sizes, such as eta-squared or partial eta-squared, which describe the proportion of variance attributable to different factors or effects. These measures help quantify the magnitude of differences between groups.\n",
    "\n",
    "3. Improving Experimental Design: By identifying the sources of variance, researchers can refine their experimental designs to reduce within-group variance and enhance the sensitivity of the study to detect between-group differences.\n",
    "\n",
    "4. Assumption Checking: It facilitates the assessment of assumptions underlying ANOVA, such as homogeneity of variance. Identifying substantial discrepancies in variances among groups prompts researchers to consider alternative analytical approaches or transformations.\n",
    "\n",
    "In essence, understanding how variance is partitioned in ANOVA provides insight into the distribution of variability in the data, aiding in result interpretation, effect size estimation, and enhancing the overall validity and reliability of statistical analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf627ec-0412-45af-a22a-8815af47416b",
   "metadata": {},
   "source": [
    "Q4. How would you calculate the total sum of squares (SST), explained sum of squares (SSE), and residual\n",
    "sum of squares (SSR) in a one-way ANOVA using Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ade447-bc7f-4df2-a59c-21113cc27542",
   "metadata": {},
   "outputs": [],
   "source": [
    "In a one-way ANOVA, the Total Sum of Squares (SST), Explained Sum of Squares (SSE), and Residual Sum of Squares (SSR) can be calculated using Python's statistical libraries like SciPy or NumPy.\n",
    "\n",
    "Here's an outline of how you can calculate these sums of squares:\n",
    "\n",
    "Assuming you have groups/levels denoted as `group1`, `group2`, ..., `groupN`, and `data` is a list or array containing the data points:\n",
    "\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "\n",
    "# Calculate the grand mean (overall mean)\n",
    "grand_mean = np.mean(data)\n",
    "\n",
    "# Calculate Total Sum of Squares (SST)\n",
    "SST = np.sum((data - grand_mean) ** 2)\n",
    "\n",
    "# Calculate group-wise means\n",
    "group_means = [np.mean(group) for group in [group1, group2, ..., groupN]]\n",
    "\n",
    "# Calculate the Explained Sum of Squares (SSE)\n",
    "SSE = np.sum([len(group) * (group_mean - grand_mean) ** 2 for group, group_mean in zip([group1, group2, ..., groupN], group_means)])\n",
    "\n",
    "# Calculate the Residual Sum of Squares (SSR)\n",
    "SSR = SST - SSE\n",
    "\n",
    "This calculation involves:\n",
    "\n",
    "- `SST`: The total variance in the data, measuring the deviation of each data point from the grand mean.\n",
    "- `SSE`: The variance explained by the differences between group means and the grand mean.\n",
    "- `SSR`: The unexplained variance, the difference between `SST` and `SSE`.\n",
    "\n",
    "Ensure the data is structured appropriately with distinct groups and adjust the code accordingly. The calculations rely on the formulas derived from the sum of squares decomposition in ANOVA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60aed956-3e10-4f11-98b9-11d77376049a",
   "metadata": {},
   "source": [
    "Q5. In a two-way ANOVA, how would you calculate the main effects and interaction effects using Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c746cd46-57e6-4e6f-be6a-ce551d29b5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "In a two-way ANOVA, the main effects and interaction effects can be calculated using Python's statistical libraries like `statsmodels` or `scipy`.\n",
    "\n",
    "Here's an example using `statsmodels` to calculate main effects and interaction effects:\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Assuming you have data in a DataFrame 'df' with columns 'factor1', 'factor2', and 'response_variable'\n",
    "model = ols('response_variable ~ factor1 + factor2 + factor1:factor2', data=df).fit()\n",
    "\n",
    "# Calculate main effects\n",
    "main_effects = sm.stats.anova_lm(model, typ=2)['sum_sq'][:-1]\n",
    "\n",
    "# Calculate interaction effect\n",
    "interaction_effect = sm.stats.anova_lm(model, typ=2)['sum_sq'][-1]\n",
    "\n",
    "print(\"Main Effects:\")\n",
    "print(main_effects)\n",
    "print(\"Interaction Effect:\")\n",
    "print(interaction_effect)\n",
    "\n",
    "This code performs a two-way ANOVA using the `statsmodels` library and computes the main effects for `factor1` and `factor2`, as well as the interaction effect between these factors. Adjust the column names and formula in the `ols()` function to match your dataset.\n",
    "\n",
    "- `factor1` and `factor2` are the categorical independent variables or factors in the two-way ANOVA.\n",
    "- `response_variable` is the dependent variable or the variable you are analyzing.\n",
    "- `typ=2` in `sm.stats.anova_lm()` calculates the Type-II sums of squares.\n",
    "\n",
    "This example assumes the use of a linear model in `statsmodels` for ANOVA, and it requires the data to be structured appropriately in a pandas DataFrame. Adjust the code based on your dataset and the specific factors you're investigating in your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bf408e-6dfb-42a4-8f5a-c4dd80428809",
   "metadata": {},
   "source": [
    "Q6. Suppose you conducted a one-way ANOVA and obtained an F-statistic of 5.23 and a p-value of 0.02.\n",
    "What can you conclude about the differences between the groups, and how would you interpret these\n",
    "results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3254da50-3088-414e-89ef-af808b02b086",
   "metadata": {},
   "outputs": [],
   "source": [
    "With an F-statistic of 5.23 and a p-value of 0.02 from a one-way ANOVA, it suggests that there are statistically significant differences between at least some of the groups. Here's the interpretation:\n",
    "\n",
    "- F-statistic: Indicates whether there are significant differences among group means. A higher F-statistic suggests greater differences between group means relative to differences within groups.\n",
    "\n",
    "- p-value: The probability of observing an F-statistic as extreme as the one computed, assuming the null hypothesis (no differences between group means) is true. A p-value of 0.02 indicates that there's a 2% chance of observing these results if there were no true differences between group means.\n",
    "\n",
    "Interpretation:\n",
    "- With a p-value of 0.02 (less than the typical alpha level of 0.05), there's enough evidence to reject the null hypothesis that all group means are equal.\n",
    "- Therefore, you'd conclude that there are statistically significant differences between at least one pair of groups.\n",
    "\n",
    "However, it doesn’t specify which groups differ or the direction of the differences. For that, further post-hoc tests or pairwise comparisons would be needed to identify which specific groups are different from each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1ab36f-ce7d-4fa4-9528-15ba6e1581f4",
   "metadata": {},
   "source": [
    "Q7. In a repeated measures ANOVA, how would you handle missing data, and what are the potential\n",
    "consequences of using different methods to handle missing data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c8d7a5-04d6-4331-a8f8-b281aabe4a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "In a repeated measures ANOVA, missing data can pose challenges. Handling missing data appropriately is crucial as it can impact the validity and reliability of the analysis. Here's how missing data can be handled and the potential consequences of different methods:\n",
    "\n",
    "Handling Missing Data:\n",
    "\n",
    "1. Complete Case Analysis (Listwise Deletion): This approach excludes all cases with missing data in any variable involved in the analysis. While it's simple, it can lead to reduced sample sizes, loss of statistical power, and potential bias if the missingness is not completely random.\n",
    "\n",
    "2. Mean/Median Imputation: Replace missing values with the mean or median of the observed values for that variable. It's simple but can distort the true variability and relationships in the data, impacting the estimates and standard errors.\n",
    "\n",
    "3. Multiple Imputation: Impute missing values multiple times based on observed data and model assumptions. This method provides more realistic estimates of uncertainty by creating multiple plausible datasets. It's more complex but accounts for the uncertainty due to missing data.\n",
    "\n",
    "4. Mixed Effects Models: Incorporate all available data using models that handle missingness inherently, such as mixed effects models. These models can estimate parameters while accounting for missing data patterns, making more efficient use of available information.\n",
    "\n",
    "Potential Consequences:\n",
    "\n",
    "- Bias: Complete case analysis may introduce bias if missingness is related to the outcome or predictors.\n",
    "  \n",
    "- Reduced Power: Complete case analysis reduces the sample size, potentially reducing the statistical power to detect effects.\n",
    "\n",
    "- Imputation Impact: Imputation methods like mean imputation can distort distributions and correlations, affecting the accuracy of estimates and hypothesis testing.\n",
    "\n",
    "- Assumptions Violation: Imputation methods assume that missing data are Missing Completely At Random (MCAR) or Missing At Random (MAR). Violation of these assumptions can lead to biased results.\n",
    "\n",
    "- Complexity: Methods like multiple imputation and mixed effects models are more complex to implement and might require assumptions about the missing data mechanism.\n",
    "\n",
    "Choosing the appropriate method should depend on the extent and pattern of missing data, the assumptions made, and the robustness of the analysis method to missingness. It's often advisable to explore sensitivity analyses using different missing data handling methods to assess the robustness of the findings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422ba6c8-6865-4aa3-b075-b34d8e2c3570",
   "metadata": {},
   "source": [
    "Q8. What are some common post-hoc tests used after ANOVA, and when would you use each one? Provide\n",
    "an example of a situation where a post-hoc test might be necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e0aefa-6466-4dd2-8e61-d6fe8198212b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Post-hoc tests are performed after ANOVA to determine specific differences between groups when the ANOVA result indicates a significant difference among groups. Commonly used post-hoc tests include Tukey's Honestly Significant Difference (HSD), Bonferroni correction, Scheffe's test, and Dunnett's test, among others.\n",
    "\n",
    "1. Tukey's Honestly Significant Difference (HSD):** Tukey's test is used to identify which specific groups differ from each other when comparing multiple means. It's helpful when the number of comparisons is relatively large. For example, after conducting an ANOVA for the effect of different teaching methods on exam scores in multiple groups, Tukey's HSD can identify which specific pairs of teaching methods resulted in significantly different scores.\n",
    "\n",
    "2. Bonferroni Correction: This method adjusts the significance level to account for multiple comparisons. It's more conservative, reducing the chance of Type I errors. Bonferroni correction divides the desired alpha level by the number of comparisons being made. For instance, in a clinical trial with multiple treatment arms, if ANOVA indicates a difference in treatment effectiveness, Bonferroni correction can be used to compare pairs of treatments while controlling for the increased risk of false positives.\n",
    "\n",
    "3. Scheffe's Test:** Scheffe's test is more conservative and is particularly useful when there's a smaller number of comparisons or unequal sample sizes among groups. It controls the family-wise error rate across all possible comparisons, making it suitable for various scenarios, especially when the number of comparisons is relatively small.\n",
    "\n",
    "4. Dunnett's Test:** Dunnett's test compares treatment groups to a control group or a reference group. It's appropriate when there's a single control group and multiple treatment groups. For instance, in pharmaceutical trials, if ANOVA reveals differences among several drug treatments and a placebo control, Dunnett's test can help identify which treatments significantly differ from the control.\n",
    "\n",
    "When to Use Post-hoc Tests:\n",
    "Post-hoc tests are necessary when ANOVA results indicate a significant difference among groups but don't specify which specific groups differ. They help in pairwise comparisons to pinpoint where those differences lie. It's crucial to use these tests to avoid making incorrect conclusions about group differences.\n",
    "\n",
    "The choice of post-hoc test often depends on the specific research question, the number of groups being compared, the nature of the comparisons, and considerations about the overall Type I error rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3589b214-a1fa-4745-89cb-cc8b1f6cef44",
   "metadata": {},
   "source": [
    "Q9. A researcher wants to compare the mean weight loss of three diets: A, B, and C. They collect data from\n",
    "50 participants who were randomly assigned to one of the diets. Conduct a one-way ANOVA using Python\n",
    "to determine if there are any significant differences between the mean weight loss of the three diets.\n",
    "Report the F-statistic and p-value, and interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd6bebc-ae68-4256-ba16-189a01f5ba22",
   "metadata": {},
   "outputs": [],
   "source": [
    "To perform a one-way ANOVA in Python using the `scipy` library, you can use the `f_oneway` function. Here's an example assuming you have weight loss data for diets A, B, and C:\n",
    "\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Weight loss data for the three diets\n",
    "weight_loss_A = [3, 4, 5, 3, 4, 5, 6, 4, 5, 6, 7, 8, 3, 4, 5, 6, 3, 4, 5, 4, 5, 6, 7, 8, 9, 10, 4, 5, 6, 7, 8]\n",
    "weight_loss_B = [2, 3, 4, 3, 4, 5, 4, 5, 6, 7, 8, 3, 4, 5, 6, 3, 4, 5, 4, 5, 6, 7, 8, 9, 10, 4, 5, 6, 7, 8, 3]\n",
    "weight_loss_C = [1, 2, 3, 2, 3, 4, 3, 4, 5, 6, 7, 8, 3, 4, 5, 6, 3, 4, 5, 4, 5, 6, 7, 8, 9, 10, 4, 5, 6, 7, 8, 3]\n",
    "\n",
    "# Perform one-way ANOVA\n",
    "f_statistic, p_value = f_oneway(weight_loss_A, weight_loss_B, weight_loss_C)\n",
    "\n",
    "# Display results\n",
    "print(\"F-statistic:\", f_statistic)\n",
    "print(\"p-value:\", p_value)\n",
    "\n",
    "# Interpretation\n",
    "alpha = 0.05  # Significance level\n",
    "if p_value < alpha:\n",
    "    print(\"There is a significant difference between the mean weight loss of the three diets.\")\n",
    "else:\n",
    "    print(\"There is no significant difference between the mean weight loss of the three diets.\")\n",
    "\n",
    "Replace the sample data in `weight_loss_A`, `weight_loss_B`, and `weight_loss_C` with your actual data for each diet. The output will give you the F-statistic and p-value. The interpretation depends on the significance level chosen (commonly 0.05). If the p-value is less than the significance level, it suggests there's a significant difference between the mean weight loss of the diets.\n",
    "\n",
    "This analysis assumes the data meets the assumptions of ANOVA, including normality, homogeneity of variances, and independence of observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae112ac-393a-471b-ab5a-c828f28835cc",
   "metadata": {},
   "source": [
    "Q10. A company wants to know if there are any significant differences in the average time it takes to\n",
    "complete a task using three different software programs: Program A, Program B, and Program C. They\n",
    "randomly assign 30 employees to one of the programs and record the time it takes each employee to\n",
    "complete the task. Conduct a two-way ANOVA using Python to determine if there are any main effects or\n",
    "interaction effects between the software programs and employee experience level (novice vs.\n",
    "experienced). Report the F-statistics and p-values, and interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d042c03-20fd-4526-bf2d-435772a86f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Certainly! Conducting a two-way ANOVA in Python involves using the `statsmodels` library. Below is an example of how you can perform a two-way ANOVA on the given data:\n",
    "\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Sample data (replace this with your actual data)\n",
    "data = {\n",
    "    'Software': ['A']*10 + ['B']*10 + ['C']*10 + ['A']*10 + ['B']*10 + ['C']*10,\n",
    "    'Experience': ['Novice']*15 + ['Experienced']*15 + ['Novice']*15 + ['Experienced']*15,\n",
    "    'Time': [23, 25, 22, 24, 26, 21, 27, 24, 22, 25,\n",
    "             28, 30, 27, 29, 31, 20, 23, 21, 25, 26,\n",
    "             22, 24, 21, 23, 25, 30, 32, 29, 31, 28]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Fit the ANOVA model\n",
    "model = ols('Time ~ C(Software) + C(Experience) + C(Software):C(Experience)', data=df).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "# Print ANOVA table\n",
    "print(anova_table)\n",
    "\n",
    "Replace the `data` dictionary with your actual dataset. This example assumes you have columns named 'Software', 'Experience', and 'Time' in your DataFrame, where 'Software' represents the software program (A, B, C), 'Experience' denotes employee experience level (Novice, Experienced), and 'Time' is the time taken to complete the task.\n",
    "\n",
    "The output will provide an ANOVA table showing the F-statistics, p-values, and interpretation. The interpretation involves examining the main effects of software, experience level, and the interaction effect between them. A significant p-value in the table indicates a significant effect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cb605c-91c0-4a52-90f7-2f7b9a17f32c",
   "metadata": {},
   "source": [
    "Q11. An educational researcher is interested in whether a new teaching method improves student test\n",
    "scores. They randomly assign 100 students to either the control group (traditional teaching method) or the\n",
    "experimental group (new teaching method) and administer a test at the end of the semester. Conduct a\n",
    "two-sample t-test using Python to determine if there are any significant differences in test scores\n",
    "between the two groups. If the results are significant, follow up with a post-hoc test to determine which\n",
    "group(s) differ significantly from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9ef337-875d-4f63-b1ac-5b182a8bdace",
   "metadata": {},
   "outputs": [],
   "source": [
    "Absolutely, here's an example of conducting a two-sample t-test followed by a post-hoc test using Python:\n",
    "\n",
    "For the two-sample t-test:\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Sample data (replace this with your actual data)\n",
    "data = {\n",
    "    'Group': ['Control']*50 + ['Experimental']*50,\n",
    "    'Scores': [78, 82, 79, 85, 75, 80, 88, 90, 76, 81,\n",
    "               92, 85, 87, 80, 83, 79, 86, 75, 78, 81,\n",
    "               89, 85, 82, 84, 79, 90, 92, 78, 85, 87,\n",
    "               95, 88, 87, 82, 83, 81, 89, 84, 82, 81,\n",
    "               78, 80, 85, 86, 83, 80, 81, 78, 79, 90]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Separate groups\n",
    "control_group = df[df['Group'] == 'Control']['Scores']\n",
    "experimental_group = df[df['Group'] == 'Experimental']['Scores']\n",
    "\n",
    "# Perform two-sample t-test\n",
    "t_stat, p_value = ttest_ind(control_group, experimental_group)\n",
    "print(f\"t-statistic: {t_stat}, p-value: {p_value}\")\n",
    "```\n",
    "\n",
    "This snippet performs the two-sample t-test on the 'Scores' column of the DataFrame. Replace the 'data' dictionary with your actual dataset.\n",
    "\n",
    "For the post-hoc test to determine which groups differ significantly:\n",
    "You may use additional post-hoc tests like Tukey's HSD test or Bonferroni correction to find pairwise differences between groups that might be significantly different. Here's an example using the `statsmodels` library:\n",
    "\n",
    "from statsmodels.stats.multicomp import MultiComparison\n",
    "\n",
    "# Create MultiComparison object\n",
    "mc = MultiComparison(df['Scores'], df['Group'])\n",
    "\n",
    "# Perform Tukey's HSD test for post-hoc analysis\n",
    "result = mc.tukeyhsd()\n",
    "print(result)\n",
    "\n",
    "This code snippet demonstrates how to perform Tukey's HSD test to identify significant differences between the control and experimental groups. Adjust the code based on your dataset and required adjustments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c8a323-111c-44bf-8826-1b3f95ff1018",
   "metadata": {},
   "source": [
    "Q12. A researcher wants to know if there are any significant differences in the average daily sales of three\n",
    "retail stores: Store A, Store B, and Store C. They randomly select 30 days and record the sales for each store\n",
    "on those days. Conduct a repeated measures ANOVA using Python to determine if there are any\n",
    "\n",
    "significant differences in sales between the three stores. If the results are significant, follow up with a post-\n",
    "hoc test to determine which store(s) differ significantly from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d167d7-3c83-4bba-a172-3a68b9b2e70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "For a repeated measures ANOVA followed by a post-hoc test, you'd generally use repeated measures ANOVA when dealing with measurements taken from the same subjects across different conditions or time points. If the data for the stores was collected on the same days, it might not be suited for a repeated measures ANOVA.\n",
    "\n",
    "However, assuming the data aligns with the repeated measures design, here's a generalized Python example using the `pingouin` library:\n",
    "\n",
    "First, install the library if you haven't already:\n",
    "```bash\n",
    "pip install pingouin\n",
    "```\n",
    "\n",
    "Here's an example of how you could perform a repeated measures ANOVA followed by a post-hoc test:\n",
    "\n",
    "import pandas as pd\n",
    "import pingouin as pg\n",
    "\n",
    "# Sample data (replace this with your actual data)\n",
    "data = {\n",
    "    'Day': [1, 2, 3, 4, 5]*3,\n",
    "    'Store': ['Store A']*5 + ['Store B']*5 + ['Store C']*5,\n",
    "    'Sales': [100, 110, 105, 98, 115,\n",
    "              95, 105, 110, 100, 108,\n",
    "              98, 100, 105, 97, 110]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Perform repeated measures ANOVA\n",
    "aov = pg.rm_anova(dv='Sales', within='Day', subject='Store', data=df)\n",
    "print(aov)\n",
    "\n",
    "# Post-hoc pairwise comparisons using Tukey's HSD test\n",
    "posthoc = pg.pairwise_ttests(dv='Sales', within='Day', subject='Store', data=df, parametric=True, padjust='holm')\n",
    "print(posthoc)\n",
    "\n",
    "This code snippet demonstrates how to perform a repeated measures ANOVA followed by a Tukey's HSD post-hoc test using the `pingouin` library. Replace the 'data' dictionary with your actual dataset. Adjust the parameters as needed for your specific data and research design."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
